{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261504c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/students/wli/UniHeidelberg/semster2/final_projects/intro2NN/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5efa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you want to use customized attention module for attention weights extraction before softmax\n",
    "from transformers.models.qwen3 import modeling_qwen3\n",
    "from modeling_qwen_attention import Qwen3Attention_v1\n",
    "modeling_qwen3.Qwen3Attention = Qwen3Attention_v1\n",
    "from reward_model import OutcomeRewardModel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a726a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = None\n",
    "\n",
    "# Load Qwen3 tokenizer and model\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model_path = \"/home/students/wli/UniHeidelberg/semster2/final_projects/models/Qwen3-0.6B-Base\"\n",
    "# model_path = model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map = \"auto\", attn_implementation=\"eager\")  # Specify eager for attention before softmax\n",
    "\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# Make sure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb83179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained reward model\n",
    "import torch\n",
    "\n",
    "# Initialize the OutcomeRewardModel with the base model\n",
    "orm = OutcomeRewardModel(model)\n",
    "\n",
    "# Load the saved state dict\n",
    "orm.load_state_dict(torch.load(\"./reward_model.pt\", map_location=orm.device))\n",
    "orm.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Reward model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b949e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.bos_token is None:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8675cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films. Label: themselves themselves themselves themselves themselves themselves themselves themselves themselves themselves\n"
     ]
    }
   ],
   "source": [
    "# test the behavior of LLM before fine tuing\n",
    "prompt = \"Sentence: a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films. Label:\"\n",
    "# prompts = [\"test prompt how old are \", \"test prompt 2 hello world\"]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# You can control max_length or early stopping\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,  # adjust based on expected label length\n",
    "    do_sample=False,  # greedy decoding\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "\n",
    "# Decode generated tokens\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc68f9",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e697f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokenizer(\"hell world\", return_tensors='pt').to(model.device), output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09972eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[56095,  1879]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hell world\", return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4211206",
   "metadata": {},
   "outputs": [],
   "source": [
    "orm = OutcomeRewardModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d16a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.620408773422241"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "orm(**tokenizer(\"i am a good man\", return_tensors = 'pt', padding = True).to(orm.device)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Anthropic/hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "epoch = 1\n",
    "eval_step = 1000\n",
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 1.0\n",
    "gradient_accumulation_steps = 1  # Increase if you still have memory issues\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(orm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Enable mixed precision training for memory efficiency\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Optimized collate function\"\"\"\n",
    "    chosen_input = tokenizer(\n",
    "        [ex['chosen'] for ex in examples], \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "        truncation=True,  # Add truncation to limit sequence length\n",
    "        max_length=512  # Adjust based on your needs\n",
    "    )\n",
    "    rejected_input = tokenizer(\n",
    "        [ex['rejected'] for ex in examples], \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return chosen_input, rejected_input\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "orm.train()  # Ensure model is in training mode\n",
    "\n",
    "for epoch_idx in range(epoch):\n",
    "    dataloader = DataLoader(\n",
    "        ds['train'], \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,  # Faster data transfer to GPU\n",
    "        num_workers=2  # Parallel data loading\n",
    "    )\n",
    "    \n",
    "    for batch_idx, (chosen_input, rejected_input) in enumerate(tqdm(dataloader, leave=True)):\n",
    "        # Move inputs to device\n",
    "        chosen_input = {k: v.to(orm.device) for k, v in chosen_input.items()}\n",
    "        rejected_input = {k: v.to(orm.device) for k, v in rejected_input.items()}\n",
    "\n",
    "        break\n",
    "        \n",
    "        # Use mixed precision training\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Forward pass\n",
    "            reward_chosen = orm(**chosen_input)\n",
    "            reward_rejected = orm(**rejected_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            diff = reward_chosen - reward_rejected\n",
    "            loss = torch.nn.functional.softplus(-diff).mean()\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update weights only after accumulation steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(orm.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Evaluation logging\n",
    "            if step % eval_step == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n",
    "        \n",
    "        # Critical: Delete tensors and clear cache periodically\n",
    "        del chosen_input, rejected_input, reward_chosen, reward_rejected, diff, loss\n",
    "        \n",
    "        if batch_idx % 50 == 0:  # Clear cache every 50 batches\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Save model\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "epoch = 1\n",
    "eval_step = 1000\n",
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 1.0\n",
    "gradient_accumulation_steps = 1  # Increase if you still have memory issues\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(orm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Enable mixed precision training for memory efficiency\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Optimized collate function\"\"\"\n",
    "    chosen_input = tokenizer(\n",
    "        [ex['chosen'] for ex in examples], \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "        truncation=True,  # Add truncation to limit sequence length\n",
    "        max_length=512  # Adjust based on your needs\n",
    "    )\n",
    "    rejected_input = tokenizer(\n",
    "        [ex['rejected'] for ex in examples], \n",
    "        return_tensors='pt', \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return chosen_input, rejected_input\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "orm.train()  # Ensure model is in training mode\n",
    "\n",
    "for epoch_idx in range(epoch):\n",
    "    dataloader = DataLoader(\n",
    "        ds['train'], \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,  # Faster data transfer to GPU\n",
    "        num_workers=2  # Parallel data loading\n",
    "    )\n",
    "    \n",
    "    for batch_idx, (chosen_input, rejected_input) in enumerate(tqdm(dataloader, leave=True)):\n",
    "        # Move inputs to device\n",
    "        chosen_input = {k: v.to(orm.device) for k, v in chosen_input.items()}\n",
    "        rejected_input = {k: v.to(orm.device) for k, v in rejected_input.items()}\n",
    "\n",
    "        break\n",
    "        \n",
    "        # Use mixed precision training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            reward_chosen = orm(**chosen_input)\n",
    "            reward_rejected = orm(**rejected_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            diff = reward_chosen - reward_rejected\n",
    "            loss = torch.nn.functional.softplus(-diff).mean()\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update weights only after accumulation steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(orm.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Evaluation logging\n",
    "            if step % eval_step == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n",
    "        \n",
    "        # Critical: Delete tensors and clear cache periodically\n",
    "        del chosen_input, rejected_input, reward_chosen, reward_rejected, diff, loss\n",
    "        \n",
    "        if batch_idx % 50 == 0:  # Clear cache every 50 batches\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Save model with proper path handling\n",
    "import os\n",
    "\n",
    "save_path = \"../reward_model.pt\"\n",
    "save_dir = os.path.dirname(save_path)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if save_dir and not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "torch.save(orm.state_dict(), save_path)\n",
    "print(\"Training completed and model saved!\")\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486ec87",
   "metadata": {},
   "source": [
    "# train rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "from torch.optim import AdamW\n",
    "eval_step = 1000 \n",
    "optimizer = AdamW(orm.parameters(), lr = 1e-5)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # print(examples)\n",
    "    chosen_input = tokenizer([ex['chosen'] for ex in examples], return_tensors = 'pt', padding = True)\n",
    "    rejected_input = tokenizer([ex['rejected']for ex in examples], return_tensors = 'pt', padding = True)\n",
    "\n",
    "    return chosen_input, rejected_input\n",
    "\n",
    "step = 0\n",
    "for i in range(epoch):\n",
    "    for chosen_input, rejected_input in tqdm(DataLoader(ds['train'], batch_size = 4, collate_fn = collate_fn), leave = True):\n",
    "        # print(batch_data[0])\n",
    "        reward_chosen = orm(**chosen_input.to(orm.device))\n",
    "        reward_rejected = orm(**rejected_input.to(orm.device))\n",
    "\n",
    "        diff = reward_chosen - reward_rejected\n",
    "        loss = torch.nn.functional.softplus(-diff).mean() # negetive log sigmoid(diff)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(orm.parameters(), 1.0)\n",
    "        optimizer.step()    \n",
    "\n",
    "        step += 1\n",
    "        if step % eval_step == 0:\n",
    "            print(f\"loss is {loss.item()}\")\n",
    "\n",
    "torch.save(orm.state_dict(), \"../reward_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d75b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8a712f",
   "metadata": {},
   "source": [
    "# test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e059b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3a57ec7",
   "metadata": {},
   "source": [
    "# test best of N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell\n",
    "MAX_LEN = 200\n",
    "def score(prompt, response):\n",
    "    seq = (prompt + tokenizer.eos_token + response).strip()\n",
    "    enc = tokenizer([seq], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_LEN).to(orm.device)\n",
    "    with torch.no_grad():\n",
    "        return orm(enc[\"input_ids\"], enc[\"attention_mask\"]).item()\n",
    "\n",
    "prompt = \"Q: How to boil an egg?\"\n",
    "cands = [\"Boil 10 min then cool.\",\"Just dunk it.\"]\n",
    "for c in cands:\n",
    "    print(score(prompt, c), c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlocal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f51a51",
   "metadata": {},
   "source": [
    "# integrate trained reward model into PPO pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45404274",
   "metadata": {},
   "source": [
    "test the model before and after PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7701934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173faa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: PPO setup using TRL\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch\n",
    "import numpy as np\n",
    "\n",
    "# Create policy model for PPO (value head wrapper)\n",
    "policy_name = 'Qwen/Qwen3-0.6B' # the same as reward model \n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(policy_name)\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "policy = AutoModelForCausalLMWithValueHead.from_pretrained(policy_name).to(DEVICE)\n",
    "ref_model = create_reference_model(policy)  # frozen reference\n",
    "\n",
    "# PPO config (tune these)\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=4,\n",
    "    forward_batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=None,\n",
    "    minibatch_size=1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_config, policy, ref_model, tokenizer=policy_tokenizer, dataset=None)  # dataset not required here\n",
    "\n",
    "# Reward wrapper that uses our trained RM\n",
    "def rm_reward_function(prompts, responses, batch_size=8):\n",
    "    # prompts: list[str], responses: list[str]\n",
    "    seqs = [p + policy_tokenizer.eos_token + r for p,r in zip(prompts, responses)]\n",
    "    # batch scoring\n",
    "    all_scores = []\n",
    "    for i in range(0, len(seqs), batch_size):\n",
    "        enc = tokenizer(seqs[i:i+batch_size], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_LEN).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            sc = rm(enc[\"input_ids\"], enc[\"attention_mask\"]).cpu().numpy()\n",
    "        all_scores.append(sc)\n",
    "    all_scores = np.concatenate(all_scores, axis=0)\n",
    "    # normalize per-batch (important for PPO stability)\n",
    "    all_scores = (all_scores - all_scores.mean()) / (all_scores.std() + 1e-8)\n",
    "    return all_scores\n",
    "\n",
    "# Example PPO step loop (toy)\n",
    "queries = [\"Write a polite greeting.\", \"Give a short tip about productivity.\"]  # prompts\n",
    "# generate responses with the policy (sampled)\n",
    "responses = []\n",
    "for q in queries:\n",
    "    encoded_q = policy_tokenizer(q, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = policy.generate(**encoded_q, do_sample=True, top_k=50, top_p=0.95, max_new_tokens=64)\n",
    "    gen_text = policy_tokenizer.decode(out[0][encoded_q[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    responses.append(gen_text)\n",
    "\n",
    "# compute rewards using RM\n",
    "rewards = rm_reward_function(queries, responses)\n",
    "# Run a PPO step\n",
    "# The TRL PPOTrainer expects tokenized tensors; the step API will accept raw texts in recent versions:\n",
    "stats = ppo_trainer.step(queries, responses, rewards)\n",
    "ppo_trainer.log_stats(stats, queries, rewards)\n",
    "print(\"PPO step done, stats:\", stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nn",
   "language": "python",
   "name": "intro2nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
